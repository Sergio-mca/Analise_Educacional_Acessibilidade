# -*- coding: utf-8 -*-
"""AnaliseEducacional_TCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynv2bOSivDNQEI_93D_h_J5oDoXyJzmm
"""

# ===========================================
# üß© PROJETO: Evas√£o Escolar com PySpark
# Fonte: INEP - Censo Escolar da Educa√ß√£o B√°sica 2024
# ===========================================

# 1Ô∏è‚É£ Instalar e configurar PySpark
!apt-get install openjdk-11-jdk -qq > /dev/null
!pip install pyspark --quiet

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg, count

# Criar sess√£o Spark
spark = SparkSession.builder \
    .appName("EvasaoEscolarINEP2024") \
    .getOrCreate()

print("‚úÖ Spark iniciado com sucesso!")

from google.colab import drive
drive.mount('/content/drive')

# ===========================================
# 2Ô∏è‚É£ IMPORTAR O ARQUIVO (duas op√ß√µes)
# ===========================================

zip_path = '/content/drive/MyDrive/Data Apoio/Projeto_Evasao_Escolar/microdados_censo_escolar_2024.zip'

# ===========================================
# 3Ô∏è‚É£ Extrair o ZIP
# ===========================================
import zipfile, os

extract_dir = "/content/censo_escolar_2024"
os.makedirs(extract_dir, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("‚úÖ Arquivo extra√≠do com sucesso!")

# ===========================================
# 4Ô∏è‚É£ Localizar o arquivo principal do Censo Escolar 2024
# ===========================================
import os

# O novo arquivo principal chama-se 'microdados_ed_basica_2024.csv'
csv_path = "/content/censo_escolar_2024/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv"

if os.path.exists(csv_path):
    print(f"‚úÖ Arquivo encontrado com sucesso:\n{csv_path}")
else:
    print("‚ùå Arquivo n√£o encontrado. Verifique se o caminho acima est√° correto.")

# ===========================================
# 5Ô∏è‚É£ Ler o CSV no PySpark
# ===========================================
df = spark.read.csv(
    csv_path,
    header=True,
    sep=';',
    inferSchema=True,
    encoding='latin1'
)

print(f"‚úÖ Dados carregados com sucesso! Linhas: {df.count()} | Colunas: {len(df.columns)}")
df.printSchema()
df.show(5)

# ===========================================
# üîç Diagn√≥stico: Inspecionar o arquivo 2024
# ===========================================

# Mostrar a quantidade total de colunas e as primeiras 50
print(f"Total de colunas: {len(df.columns)}\n")
print("Primeiras 50 colunas:\n")
for i, col_name in enumerate(df.columns[:50], start=1):
    print(f"{i:02d}. {col_name}")

# Mostrar 5 linhas com essas colunas
df.select(df.columns[:10]).show(5, truncate=False)

# ===========================================
# üîç Procurar colunas relevantes (idade, aluno, ensino)
# ===========================================
cols_relevantes = [c for c in df.columns if any(x in c.upper() for x in ["ALUNO", "IDADE", "ENSINO"])]
print(f"Total de colunas relevantes encontradas: {len(cols_relevantes)}\n")
for i, c in enumerate(cols_relevantes, start=1):
    print(f"{i:02d}. {c}")

import os

dados_dir = "/content/censo_escolar_2024/microdados_censo_escolar_2024/dados"

print("üìÅ Listando arquivos dentro da pasta 'dados':\n")
for f in os.listdir(dados_dir):
    print(f)

# ===========================================
# üîç Buscar poss√≠veis colunas de idade, ensino e situa√ß√£o do aluno
# ===========================================

keywords = ["IDADE", "ALUNO", "MATRIC", "ENSINO", "ETAPA", "SITUAC", "MODALIDADE", "ETAPA_ENSINO"]
cols_interessantes = [c for c in df.columns if any(k in c.upper() for k in keywords)]

print(f"Total de colunas encontradas: {len(cols_interessantes)}\n")
for i, c in enumerate(cols_interessantes, start=1):
    print(f"{i:02d}. {c}")

# ===========================================
# 6Ô∏è‚É£ Limpeza e An√°lise Explorat√≥ria - Infraestrutura Escolar
# ===========================================
from pyspark.sql.functions import col, avg, sum as _sum

# Selecionar apenas colunas relevantes para infraestrutura e acessibilidade
infra_cols = [
    "SG_UF", "NO_UF",
    "IN_DESKTOP_ALUNO", "QT_DESKTOP_ALUNO",
    "IN_COMP_PORTATIL_ALUNO", "QT_COMP_PORTATIL_ALUNO",
    "IN_TABLET_ALUNO", "QT_TABLET_ALUNO",
    "IN_INTERNET_ALUNOS", "IN_INTERNET_COMUNIDADE",
    "IN_ACESSIBILIDADE_RAMPAS", "IN_ACESSIBILIDADE_CORRIMAO",
    "IN_ACESSIBILIDADE_ELEVADOR", "IN_ACESSIBILIDADE_SINAL_TATIL",
    "IN_ACESSIBILIDADE_SINAL_VISUAL", "IN_ACESSIBILIDADE_SINALIZACAO"
]

df_infra = df.select([c for c in infra_cols if c in df.columns])

# Converter colunas bin√°rias (0/1) corretamente
for c in df_infra.columns:
    if c.startswith("IN_"):
        df_infra = df_infra.withColumn(c, col(c).cast("int"))

# Verificar dados nulos
df_infra.select([(col(c).isNull().cast("int")).alias(c) for c in df_infra.columns]).summary().show()

# Estat√≠sticas gerais
df_infra.describe().show()

# ===========================================
# 7Ô∏è‚É£ An√°lise por Estado e Regi√£o
# ===========================================
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd

# Agregar por estado
df_estado = (
    df_infra.groupBy("SG_UF")
    .agg(
        F.avg("IN_INTERNET_ALUNOS").alias("pct_internet_alunos"),
        F.avg("IN_ACESSIBILIDADE_RAMPAS").alias("pct_rampas"),
        F.avg("IN_ACESSIBILIDADE_CORRIMAO").alias("pct_corrimao"),
        F.avg("IN_ACESSIBILIDADE_SINAL_VISUAL").alias("pct_sinal_visual"),
        F.avg("IN_DESKTOP_ALUNO").alias("pct_desktop_aluno")
    )
    .orderBy("SG_UF")
)

# Converter para Pandas para visualiza√ß√£o
pdf = df_estado.toPandas()

# Gr√°fico 1 ‚Äì Acesso √† Internet por Estado
plt.figure(figsize=(12,6))
plt.bar(pdf["SG_UF"], pdf["pct_internet_alunos"]*100)
plt.title("Percentual de Escolas com Internet para Alunos por Estado (2024)")
plt.xlabel("Estado")
plt.ylabel("Percentual (%)")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Gr√°fico 2 ‚Äì Acessibilidade (Rampas e Corrim√£os)
plt.figure(figsize=(12,6))
plt.plot(pdf["SG_UF"], pdf["pct_rampas"]*100, label="Rampas")
plt.plot(pdf["SG_UF"], pdf["pct_corrimao"]*100, label="Corrim√£os")
plt.title("Infraestrutura de Acessibilidade nas Escolas por Estado (2024)")
plt.xlabel("Estado")
plt.ylabel("Percentual (%)")
plt.legend()
plt.grid(True)
plt.show()

# ===========================================
# 8Ô∏è‚É£ Colunas derivadas
# ===========================================
df_infra = df_infra.withColumn(
    "QT_RECURSOS_TECNOLOGICOS",
    (col("IN_INTERNET_ALUNOS") + col("IN_DESKTOP_ALUNO") + col("IN_TABLET_ALUNO"))
)

df_infra = df_infra.withColumn(
    "QT_RECURSOS_ACESSIBILIDADE",
    (col("IN_ACESSIBILIDADE_RAMPAS") + col("IN_ACESSIBILIDADE_CORRIMAO") + col("IN_ACESSIBILIDADE_ELEVADOR"))
)

df_infra.select("SG_UF", "QT_RECURSOS_TECNOLOGICOS", "QT_RECURSOS_ACESSIBILIDADE").show(5)

# ===========================================
# 9Ô∏è‚É£ An√°lises explorat√≥rias adicionais
# ===========================================
from pyspark.sql import functions as F

# 1Ô∏è‚É£ Distribui√ß√£o geral dos recursos tecnol√≥gicos
df_infra.groupBy("SG_UF").agg(F.avg("QT_RECURSOS_TECNOLOGICOS").alias("media_tecnologia")).orderBy("media_tecnologia", ascending=False).show(10)

# 2Ô∏è‚É£ Distribui√ß√£o geral dos recursos de acessibilidade
df_infra.groupBy("SG_UF").agg(F.avg("QT_RECURSOS_ACESSIBILIDADE").alias("media_acess")).orderBy("media_acess", ascending=False).show(10)

# 3Ô∏è‚É£ Correla√ß√£o entre tecnologia e acessibilidade
df_corr = df_infra.select("QT_RECURSOS_TECNOLOGICOS", "QT_RECURSOS_ACESSIBILIDADE").toPandas()
print(df_corr.corr())

# 4Ô∏è‚É£ Identifica√ß√£o de estados com valores extremos (outliers)
pdf = df_infra.groupBy("SG_UF").agg(F.avg("QT_RECURSOS_TECNOLOGICOS").alias("media_tec")).toPandas()
print(pdf.describe())

# 5Ô∏è‚É£ M√©dia nacional de internet e acessibilidade
df_infra.select(
    F.avg("IN_INTERNET_ALUNOS").alias("media_internet"),
    F.avg("QT_RECURSOS_ACESSIBILIDADE").alias("media_acessibilidade")
).show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

pdf_viz = df_infra.groupBy("SG_UF").agg(
    F.avg("IN_INTERNET_ALUNOS").alias("internet"),
    F.avg("IN_DESKTOP_ALUNO").alias("desktop"),
    F.avg("IN_TABLET_ALUNO").alias("tablet"),
    F.avg("QT_RECURSOS_TECNOLOGICOS").alias("tec_total"),
    F.avg("QT_RECURSOS_ACESSIBILIDADE").alias("acess_total")
).toPandas()

# Gr√°fico 3 ‚Äì Comparativo geral de tecnologia
plt.figure(figsize=(12,6))
sns.barplot(data=pdf_viz, x="SG_UF", y="tec_total", color="skyblue")
plt.title("Recursos Tecnol√≥gicos M√©dios por Estado (2024)")
plt.xticks(rotation=45)
plt.show()

# Gr√°fico 4 ‚Äì Acessibilidade
plt.figure(figsize=(12,6))
sns.barplot(data=pdf_viz, x="SG_UF", y="acess_total", color="orange")
plt.title("Recursos de Acessibilidade M√©dios por Estado (2024)")
plt.xticks(rotation=45)
plt.show()

# Gr√°fico 5 ‚Äì Correla√ß√£o entre tecnologia e acessibilidade
sns.scatterplot(data=pdf_viz, x="tec_total", y="acess_total")
plt.title("Correla√ß√£o: Infraestrutura Tecnol√≥gica x Acessibilidade")
plt.xlabel("M√©dia de Recursos Tecnol√≥gicos")
plt.ylabel("M√©dia de Recursos de Acessibilidade")
plt.show()

# ===========================================
# üîÑ Exportar dataset tratado para CSV
# ===========================================
pdf_final = df_infra.toPandas()
pdf_final.to_csv("/content/infraestrutura_escolar_tratada.csv", index=False)

print("‚úÖ Arquivo exportado com sucesso!")
print("üìÇ Caminho: /content/infraestrutura_escolar_tratada.csv")